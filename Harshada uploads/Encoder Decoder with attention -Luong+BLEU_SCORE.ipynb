{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset into data_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"islcorpus.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>dad upset</td>\n",
       "      <td>dad was upset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>Good Afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>you wrong</td>\n",
       "      <td>You are wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tommorow together we go</td>\n",
       "      <td>Shall we go together tommorow?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Good question</td>\n",
       "      <td>Good question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Input                          Output\n",
       "118                dad upset                   dad was upset\n",
       "24            good afternoon                  Good Afternoon\n",
       "62                 you wrong                   You are wrong\n",
       "51   tommorow together we go  Shall we go together tommorow?\n",
       "53             Good question                   Good question"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_raw = pd.read_csv(data_path, header = 0)\n",
    "lines_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "#sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "    num_digits= str.maketrans('','', digits)\n",
    "    \n",
    "    sentence= sentence.lower()\n",
    "    sentence= re.sub(\" +\", \" \", sentence)\n",
    "    sentence= re.sub(\"'\", '', sentence)\n",
    "    sentence= sentence.translate(num_digits)\n",
    "    sentence= sentence.strip()\n",
    "    sentence= re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.rstrip().strip()\n",
    "    sentence=  'start_ ' + sentence + ' _end'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding start and end tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_ i see car _end\n",
      "b'start_ i see the car _end'\n"
     ]
    }
   ],
   "source": [
    "isl_sentence = u\"I see car\"\n",
    "eng_sentence = u\"i see the car\"\n",
    "print(preprocess_sentence(isl_sentence))\n",
    "print(preprocess_sentence(eng_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path,encoding = 'UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(',')]for l in lines[:num_examples]]\n",
    "    print(path)\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islcorpus.csv\n",
      "start_ smoke not _end\n",
      "start_ dont smoke _end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size= 300\n",
    "source, target = create_dataset(data_path, sample_size)\n",
    "print(source[-1])\n",
    "print(target[-1])\n",
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
    "source_sentence_tokenizer.fit_on_texts(source)\n",
    "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
    "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "target_sentence_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(target)\n",
    "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding = 'post')\n",
    "print(len(target_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "max_target_length= max(len(t) for t in  target_tensor)\n",
    "print(max_target_length)\n",
    "max_source_length= max(len(t) for t in  source_tensor)\n",
    "print(max_source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 25 100 25\n"
     ]
    }
   ],
   "source": [
    "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor= train_test_split(source_tensor, target_tensor,test_size=0.2)\n",
    "print(len(source_train_tensor), len(source_test_tensor), len(target_train_tensor), len(target_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 112 13 13\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size=0.1)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> start_\n",
      "66 ----> baby\n",
      "67 ----> cry\n",
      "2 ----> _end\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> start_\n",
      "82 ----> baby\n",
      "5 ----> is\n",
      "83 ----> crying\n",
      "2 ----> _end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(source_sentence_tokenizer, source_train_tensor[0])\n",
    "print ()\n",
    "\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert( target_sentence_tokenizer, target_train_tensor[0])\n",
    "type(source_train_tensor)\n",
    "type(target_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(source_train_tensor)\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(source_train_tensor)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 8]), TensorShape([16, 11]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (16, 8, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        print('\\n******* Luong Attention  STARTS******')\n",
    "        print('query (decoder hidden state): (batch_size, hidden size) ', query.shape)\n",
    "        print('values (encoder all hidden state): (batch_size, max_len, hidden size) ', values.shape)\n",
    "\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    \n",
    "        print('query_with_time_axis:(batch_size, 1, hidden size) ', query_with_time_axis.shape)\n",
    "\n",
    "\n",
    "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
    "        print('values_transposed:(batch_size, hidden size, max_len) ', values_transposed.shape)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        #BAHDANAU ADDITIVE:\n",
    "        #score = self.V(tf.nn.tanh(\n",
    "        #    self.W1(query_with_time_axis) + self.W2(values)))\n",
    "    \n",
    "        #LUONGH Dot-product\n",
    "        score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n",
    "\n",
    "        print('score: (batch_size, max_length, 1) ',score.shape)\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        print('attention_weights: (batch_size, max_length, 1) ',attention_weights.shape)\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        print('context_vector before reduce_sum: (batch_size, max_length, hidden_size) ',context_vector.shape)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        print('context_vector after reduce_sum: (batch_size, hidden_size) ',context_vector.shape)\n",
    "\n",
    "\n",
    "        print('\\n******* Luong Attention ENDS******')\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Attention result shape: (batch size, units) (16, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (16, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = LuongAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Decoder output shape: (batch_size, vocab size) (16, 184)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (16, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (16, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (16, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (16, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (16, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (16, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (16, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Epoch 1 Batch 0 loss 2.577983856201172\n",
      "Epoch 1 Loss 2.5181\n",
      "Time taken for 1 epoch 23.37338352203369 sec\n",
      "\n",
      "Epoch 2 Batch 0 loss 1.9987627267837524\n",
      "Epoch 2 Loss 2.2490\n",
      "Time taken for 1 epoch 5.790943145751953 sec\n",
      "\n",
      "Epoch 3 Batch 0 loss 2.272700548171997\n",
      "Epoch 3 Loss 2.1474\n",
      "Time taken for 1 epoch 2.304220199584961 sec\n",
      "\n",
      "Epoch 4 Batch 0 loss 1.8175697326660156\n",
      "Epoch 4 Loss 1.9338\n",
      "Time taken for 1 epoch 6.239912271499634 sec\n",
      "\n",
      "Epoch 5 Batch 0 loss 1.6639155149459839\n",
      "Epoch 5 Loss 1.8442\n",
      "Time taken for 1 epoch 2.3524105548858643 sec\n",
      "\n",
      "Epoch 6 Batch 0 loss 1.6396769285202026\n",
      "Epoch 6 Loss 1.7812\n",
      "Time taken for 1 epoch 6.093005895614624 sec\n",
      "\n",
      "Epoch 7 Batch 0 loss 1.4696327447891235\n",
      "Epoch 7 Loss 1.6644\n",
      "Time taken for 1 epoch 2.259303569793701 sec\n",
      "\n",
      "Epoch 8 Batch 0 loss 1.7110308408737183\n",
      "Epoch 8 Loss 1.6102\n",
      "Time taken for 1 epoch 3.9808573722839355 sec\n",
      "\n",
      "Epoch 9 Batch 0 loss 1.4789907932281494\n",
      "Epoch 9 Loss 1.5095\n",
      "Time taken for 1 epoch 2.2838079929351807 sec\n",
      "\n",
      "Epoch 10 Batch 0 loss 1.4840773344039917\n",
      "Epoch 10 Loss 1.3995\n",
      "Time taken for 1 epoch 4.4986255168914795 sec\n",
      "\n",
      "Epoch 11 Batch 0 loss 1.2614797353744507\n",
      "Epoch 11 Loss 1.3203\n",
      "Time taken for 1 epoch 2.3293535709381104 sec\n",
      "\n",
      "Epoch 12 Batch 0 loss 1.2103679180145264\n",
      "Epoch 12 Loss 1.2693\n",
      "Time taken for 1 epoch 4.566903114318848 sec\n",
      "\n",
      "Epoch 13 Batch 0 loss 1.0685240030288696\n",
      "Epoch 13 Loss 1.1865\n",
      "Time taken for 1 epoch 2.3362812995910645 sec\n",
      "\n",
      "Epoch 14 Batch 0 loss 1.2045878171920776\n",
      "Epoch 14 Loss 1.1031\n",
      "Time taken for 1 epoch 4.497645854949951 sec\n",
      "\n",
      "Epoch 15 Batch 0 loss 0.8566156029701233\n",
      "Epoch 15 Loss 1.0091\n",
      "Time taken for 1 epoch 2.3745110034942627 sec\n",
      "\n",
      "Epoch 16 Batch 0 loss 0.9930635094642639\n",
      "Epoch 16 Loss 0.9297\n",
      "Time taken for 1 epoch 4.616448402404785 sec\n",
      "\n",
      "Epoch 17 Batch 0 loss 0.8255950808525085\n",
      "Epoch 17 Loss 0.8410\n",
      "Time taken for 1 epoch 2.243521213531494 sec\n",
      "\n",
      "Epoch 18 Batch 0 loss 0.7511038780212402\n",
      "Epoch 18 Loss 0.7447\n",
      "Time taken for 1 epoch 4.8143086433410645 sec\n",
      "\n",
      "Epoch 19 Batch 0 loss 0.6097434163093567\n",
      "Epoch 19 Loss 0.6631\n",
      "Time taken for 1 epoch 2.2838728427886963 sec\n",
      "\n",
      "Epoch 20 Batch 0 loss 0.5386369824409485\n",
      "Epoch 20 Loss 0.6122\n",
      "Time taken for 1 epoch 4.026771306991577 sec\n",
      "\n",
      "Epoch 21 Batch 0 loss 0.5301063060760498\n",
      "Epoch 21 Loss 0.5389\n",
      "Time taken for 1 epoch 2.3373072147369385 sec\n",
      "\n",
      "Epoch 22 Batch 0 loss 0.4850497841835022\n",
      "Epoch 22 Loss 0.4764\n",
      "Time taken for 1 epoch 4.5815277099609375 sec\n",
      "\n",
      "Epoch 23 Batch 0 loss 0.42888158559799194\n",
      "Epoch 23 Loss 0.4354\n",
      "Time taken for 1 epoch 2.4058847427368164 sec\n",
      "\n",
      "Epoch 24 Batch 0 loss 0.40765416622161865\n",
      "Epoch 24 Loss 0.4143\n",
      "Time taken for 1 epoch 5.2494871616363525 sec\n",
      "\n",
      "Epoch 25 Batch 0 loss 0.3406035900115967\n",
      "Epoch 25 Loss 0.3644\n",
      "Time taken for 1 epoch 2.3998231887817383 sec\n",
      "\n",
      "Epoch 26 Batch 0 loss 0.25389188528060913\n",
      "Epoch 26 Loss 0.3264\n",
      "Time taken for 1 epoch 4.814565658569336 sec\n",
      "\n",
      "Epoch 27 Batch 0 loss 0.3100821077823639\n",
      "Epoch 27 Loss 0.2966\n",
      "Time taken for 1 epoch 2.2678258419036865 sec\n",
      "\n",
      "Epoch 28 Batch 0 loss 0.22473180294036865\n",
      "Epoch 28 Loss 0.2620\n",
      "Time taken for 1 epoch 4.324779510498047 sec\n",
      "\n",
      "Epoch 29 Batch 0 loss 0.24489407241344452\n",
      "Epoch 29 Loss 0.2229\n",
      "Time taken for 1 epoch 2.4436302185058594 sec\n",
      "\n",
      "Epoch 30 Batch 0 loss 0.21813438832759857\n",
      "Epoch 30 Loss 0.2055\n",
      "Time taken for 1 epoch 4.616588830947876 sec\n",
      "\n",
      "Epoch 31 Batch 0 loss 0.1565542370080948\n",
      "Epoch 31 Loss 0.1810\n",
      "Time taken for 1 epoch 2.3216664791107178 sec\n",
      "\n",
      "Epoch 32 Batch 0 loss 0.20230042934417725\n",
      "Epoch 32 Loss 0.1732\n",
      "Time taken for 1 epoch 7.3568220138549805 sec\n",
      "\n",
      "Epoch 33 Batch 0 loss 0.1357632577419281\n",
      "Epoch 33 Loss 0.1571\n",
      "Time taken for 1 epoch 2.598723888397217 sec\n",
      "\n",
      "Epoch 34 Batch 0 loss 0.1516464650630951\n",
      "Epoch 34 Loss 0.1646\n",
      "Time taken for 1 epoch 5.753060817718506 sec\n",
      "\n",
      "Epoch 35 Batch 0 loss 0.14798292517662048\n",
      "Epoch 35 Loss 0.1562\n",
      "Time taken for 1 epoch 2.4906976222991943 sec\n",
      "\n",
      "Epoch 36 Batch 0 loss 0.08863238990306854\n",
      "Epoch 36 Loss 0.1600\n",
      "Time taken for 1 epoch 5.315810441970825 sec\n",
      "\n",
      "Epoch 37 Batch 0 loss 0.11532620340585709\n",
      "Epoch 37 Loss 0.1233\n",
      "Time taken for 1 epoch 2.7300801277160645 sec\n",
      "\n",
      "Epoch 38 Batch 0 loss 0.09599539637565613\n",
      "Epoch 38 Loss 0.1081\n",
      "Time taken for 1 epoch 5.696589469909668 sec\n",
      "\n",
      "Epoch 39 Batch 0 loss 0.09941648691892624\n",
      "Epoch 39 Loss 0.0950\n",
      "Time taken for 1 epoch 2.2743687629699707 sec\n",
      "\n",
      "Epoch 40 Batch 0 loss 0.07255580276250839\n",
      "Epoch 40 Loss 0.0819\n",
      "Time taken for 1 epoch 4.807849168777466 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} loss {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
    "   \n",
    "      \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_target_length, max_source_length))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    #print(sentence)\n",
    "    #print(source_sentence_tokenizer.word_index)\n",
    "\n",
    "    inputs = [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_source_length,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
    "\n",
    "    for t in range(max_target_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "  \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x17a25be8640>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Input: start_ i come before _end\n",
      "Predicted translation: i have three balls _end \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-71f02795d875>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-50-71f02795d875>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'i come before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'food'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-7f246445eae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'food want'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-503961f0d96c>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted translation: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-588bb9d2715c>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(source_sentence_tokenizer.word_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msource_sentence_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[0;32m     10\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_source_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-588bb9d2715c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(source_sentence_tokenizer.word_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msource_sentence_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[0;32m     10\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_source_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'food'"
     ]
    }
   ],
   "source": [
    "translate(u'food want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Input: start_ i stay pune _end\n",
      "Predicted translation: i stay in pune _end \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-71f02795d875>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-50-71f02795d875>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'i stay pune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'bank where')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'i understand not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "\n",
      "******* Luong Attention  STARTS******\n",
      "query (decoder hidden state): (batch_size, hidden size)  (1, 1024)\n",
      "values (encoder all hidden state): (batch_size, max_len, hidden size)  (1, 8, 1024)\n",
      "query_with_time_axis:(batch_size, 1, hidden size)  (1, 1, 1024)\n",
      "values_transposed:(batch_size, hidden size, max_len)  (1, 1024, 8)\n",
      "score: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "attention_weights: (batch_size, max_length, 1)  (1, 8, 1)\n",
      "context_vector before reduce_sum: (batch_size, max_length, hidden_size)  (1, 8, 1024)\n",
      "context_vector after reduce_sum: (batch_size, hidden_size)  (1, 1024)\n",
      "\n",
      "******* Luong Attention ENDS******\n",
      "Input: start_ father clerk _end\n",
      "Predicted translation: my father is a doctor _end \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-71f02795d875>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-50-71f02795d875>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAJiCAYAAADNONnGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdNUlEQVR4nO3de7StdV3v8fcnNhfBQx40SzwgXhMyUc8OrI5okYp69JTWKJPqoLK9jaNlHus40sx0kIaJmqbbG3qOlgpmoeUVDTUvoYno9hJ3RFMIRATk+j1/PM8aTBZrb9ZebNYz53e/X2OssfZ65mV912TvN8/6zWc+M1WFJKmfH5l6AEnSrcPAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeC2EJBuSPD3JvlPPIi2K+EpWLYoklwMHVdW5U88iLQL34LVIPgM8YOohpEWxYeoBtG1JTgceWVXnTz3LHHgD8PIkdwE+D1w+e2FVfWGSqaQ55RLNnEtyGXBwVZ019SxTS3L9Ni6uqtpl3YaRFoB78Fokd516AGmRGHgtDJ9clbaPT7JqoSR5RJL3JdmSZL9x25OTHD71bNK8MfBaGEmeALwL+DeG5Zpdx4t2AZ471VzSvDLwWiTPBY6uqt8Drp3Z/hngfpNMtACSHL2Ny163nrPMsyT7r/Zj6llXyzX4iST5beCdVXXVsu27Ab9RVW8bNz0F+M56zzen7gl8eoXtPwD2XudZFsnLklxcVSfObkyyGXj4RDPNo3OA1R5WuBBHbLkHP523AD+6wvb/NF4GQFW9o6ouX+F6O6NvAfdaYfthwJnrPMsi+VXgzbPPU4xxPwL4hcmmmj8/AxwyfhzJ8PftBcBDx48XABeMly0E9+CnE1beW9gfuHSdZ1kUm4FXJXny+PV+SR4EvAx44WRTzbmq+miSJwInJDkCeDLwMOAhvr7iBlX1+aU/J/kL4Peq6oSZq5yc5OvAs4C/Xu/51sLAr7Pxlak1fvxTktm15F2AuwD/MMVs866qXpbkR4EPA3sAHwOuAo6tqtdMOtycq6oTk+wDnAJ8G3hwVZ0z7VRz7RDgSyts/xLwX9d5ljXzlazrLMkfj3/8Y+DlDOvHS65mWAc8saquXufRFkaSPYGDGJYYt1TVD27mJjudJK/aykW/DJwGnL20oaqeuR4zLZIkXwU+WFW/u2z7ccDDq+rAKebaXgZ+Akk2MDx5+t6qumDqedRPko+t8qpVVb94qw6zgMalrL8FzmU4SgvgUOAA4LFV9Y8TjbZdDPxEkvwQuLe/Jq9ekj0Y1j8PB+7IsoMEquq+U8w178bfeK6qquumnmWRJPkvwNOBezM8Z7YFeN0infjPNfjpnAbcg2FJRqvzWuBXgHcD/8zqD2nbaSXZheFJ+4MZAqVVqqpvAs+beo5bwsBP54UMp779Y1Y+9e3FUww1534Z+LWq+sjUgyyKqrouybnAblPPsmjG33zux8q/Lb5nipm2l0s0E1l26tvZ/wjBU9+uKMk3gcOr6utTz7JIkvwO8HjgyKq6aOp5FkGSX2I4FPL2K1y8MP8+DfxEkjx4W5dX1T+t1yyLIskzgZ8CnlZV2zo3vGaMh+Yunbvnm9z0t0Wfu1gmyVeAfwGeV1XfmnqetXKJZiIGfHWS/P2yTYcBRyTZAlwze0FVPWbdBlssJ9z8VbTMAcBjFjnuYOAnl2Rfhlev3miNtKpOmWaiufMfy77+20mmWGBV9SdTz7CAPgX8JAt+CgyXaCYyhv0dDHukxbJTFyzKGp8Ww3iI6X8H7g68vqq+l+TuwCU+oX9TSR4LvBj4C+B0bvrb4kK8/6+Bn0iSdzE8gfMMhrW+I4AfB17EcA6MD0843lxKcjLDi0y+t2z73gwvGvMFOytIcg/gI8BtgdsB96qqs5IcC9yuqp68rdvvjLq8/69LNNN5MPCoqvpakgIurKpPJbkK+FOG863oxh7Cyof77QE8aH1HWSjHAR8CngZ8b2b73zNz5lLdSIv3/zXw07kNsHTI2sUMx9p+g+HFKB7VMCPJA2a+vG+S2SWFXRjOae4pH7bu54AHjsfEz24/D9h3mpHmW5f3/zXw0/kaw0ugzwG+CDw1yfkMSzbG6sZO5YYzcH5ohcuvBP7Xuk60eHZdYZunpt6GJI9g+Pd4N4YTjJ0/nqr67Kr66LTTrY5v+DGdVwI/Mf75RQzn5z6L4dwXC/3y6FvBXRmeHAzDaVzvOvNxZ2DvqnrzdOPNvQ8Bz575usbnLf4EeP80I823Lu//65Osc2J8WfS9gfN8taF2pPGIraWzS94N+FeG8yB9Bzisqi6carZ5leQ04Jiq+psklwEHj09MHwx8qKp+fOIRV8XATyTJCxjeqOKKZdtvA/zvqnrRNJPNt/FUy4ew8msH3rbijbT09+rxwAMYfnP/AvD2qrpy0sHmVJIrgAOr6txlgb878OWqus3EI66KgZ9IkuuAO1XVd5dtvz3w3UU5DGs9Jbk3cBLDr8wBrmN4HukahtPh+sbb2iGSnMFwSowPLwv8UcDvV9V9Jh5xVXySdTpbe0/W+zMcVaObOo7hzJv3A/59/PyjwF8BfzTVUPNofKHOqizKmRHXWYv3/zXw62zcG1g6IuSs8Rj4JbswHNP9uilmWwA/w/BeopePL0TZUFVfSPJc4NV4eOms1Z5/phj+3mnGat//d3xTkG/N68nvXKJZZ+OpWwO8GfhdbnyY2tXAOVX16QlGm3vj8e8bx1+VzwA2VdXJ47ro6VW158Qjqpmbe//fJN8H7ldVZ00x381xD36dVdVbAZLsBZxSVaePXz8U+B3gK0k+59urrejLDO9MdBbwOeAPxucyjgbOmHKweZbkJcD5VfW6ZdufCuxbVS+YZrL5Nx4Eceo2rpJtXDY5j4OfzpEM5zZf+jXvvcA+DC+sePF0Y82XJIeNR84AvGTmoj8C9mP41flhwDPXe7YF8lsMh0Yu9wWGnQo1ZeCncyDDPzCAXwM+V1WPZPjH+PjJppo/H2P4Hx8MT6aeAlBVZ1XVQcAdgB+vqo9PM95CuCOw0rHuFzGc4E5NGfjp7MKw5g5wOPAP45/PxH90sy7hhhM/HcBN3xvz4vKJpJtzHiufjO0whnd4UlOuwU/ny8DTkryPIfD/Z9x+Z244CZngROCfknyb4YiPU8d195uoqrut62SL4/XAK5LsBpw8bjscOAZ46WRT9TDXOxcGfjp/wLDu/hzgrUtPtgKPYXgCUYOnMpzW9p4Mb77wFuCySSdaMFX18iR3AF7FDa/+vRp4ZVW9bLrJWpjrJ1k9THJCSXZhOFHWJTPbDgCuWP4KV0GStwDPrCoDvwbjkVsHMUTpJof8afsl2Y/hOPi5POrNwEtSUz7JKklNGfg5kWTT1DMsGh+ztfFxW5tFfNwM/PxYuL88c8DHbG183NZm4R43Ay9JTe1UT7Lult1rD/aaeowVXcNV7MruU4+xUHzM1sbHbW3m+XG7jEsuqqofW759pzoOfg/24tAcPvUYkrRDfaROOHel7S7RSFJTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1NSkgU/y8SR/leTlSS5OcmGSZyXZPclrknwvyXlJfmu8/slJ/nLZfeyd5Iokj53mp5Ck+TQPe/BPAC4DDgX+DDgOeC/wDWAj8FbgjUn2Bd4A/GaS3Wdu/3jgB8BJ6zeyJM2/eQj8V6rqhVX1b8BfABcB11TVK6vqDOBFQICfA94DXA/8ysztnwi8raquWenOk2xKcmqSU6/hqlv1B5GkeTIPgf/S0h+qqoDvAqfPbLsGuAS4Y1VdBfxfhqiT5CDgEODNW7vzqtpcVRurauOu7L61q0lSOxumHgBYvuddW9m29D+jNwJfSrI/8CTg01W15dYdUZIWzzzswW+XqvoK8FngaOBItrH3Lkk7s3nYg1+LNwCvY9jTf+fEs0jSXFq4PfjRO4GrgXdV1WVTDyNJ82jSPfiqesgK2+6zwrafWLbpdsBtgDfdKoNJUgMLtUSTZFfgTsBLgH+tqk9NPJIkza1FW6L5eeBchhdFHT3xLJI01xZqD76qPs7woidJ0s1YtD14SdIqGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNbVdgU/yI0len+Q/klSSh6zlmyY5Psn71nJbSdLqbO8e/COBo4BHA3cC/nlbV05ywPg/go1rnE+StEYbtvP69wC+XVXbDPtUkuxaVddMPYckzYNV78EnOR54BbD/uFd+TpIjknwiySVJLk7ywSQHztzs7PHzv4y3+fiy+3xWkgvG278lyZ4zlyXJc5OcmeTKJKcnOXLm8qXfDh6f5OQkVwJPWcNjIEktbc8e/LOAc4EnAj8DXAccBhwHfAm4DfBHwElJDqqqq4FDgM8BRwCnAVfP3N+DgG8DvwTsB7wL+AZwzHj5i4FfBZ4BfB34WeANSS6pqvfP3M8xwHOAJwE32XtPsgnYBLAHey6/WJLaWnXgq+rSJJcB11XVv4+bT5y9TpKjgO8zhP2TwIXjRf8xc5sl3weeVlXXAl9N8m7gcOCYJHsBzwYeVlWfGK9/dpJDGII/G/hXV9UJ25h7M7AZYO/sU6v9eSVp0W3vGvyNJLk78KfAocCPMSz5/Aiw/ypuvmWM+5JvjfcDcBCwB/CBJLNR3hU4Z9n9nLr9k0tSf7co8MBJwAUMa98XANcCW4DdVnHb5cspxQ3PCSx9fjRw3s3c7vLVDitJO5M1Bz7J7YEDgWdU1cfGbQ9Ydp9La+67bOfdbwGuAu5SVSevdUZJ2pndkj34S4CLgKOTnA/cGfhzhr34Jd8FrgQenuQc4IdVdenN3XFVXZbkWODYJAFOAW4LPBC4flxXlyRtw5pPVVBV1wO/DtwX+DLwGuD5DHveS9e5Fngm8GSGNfa/245v8XzghQxHyHwF+DDwOG449FKStA2p2nkOLNk7+9ShOXzqMSRph/pInfD5qrrJGQM82ZgkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpraMPUAkrTkisceOvUIi+nEE1bc7B68JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJamphQl8kuOTvG/qOSRpUWyYeoDt8CwgUw8hSYtiYQJfVZdOPYMkLZKFXKJJcliSzyT5QZJLk3w2yX2mnlGS5snC7MEvSbIB+DvgTcATgF2BBwDXTTmXJM2bhQs8sDdwO+Ckqjpz3Pa1rV05ySZgE8Ae7HmrDydJ82JhlmiWVNXFwPHAB5O8P8mzk+y3jetvrqqNVbVxV3ZftzklaWoLF3iAqjoKOBQ4BXgM8I0kD592KkmaLwsZeICqOq2qXlpVDwE+DvzOtBNJ0nxZuMAnuWuSP0vyc0nukuQXgPsCW6aeTZLmySI+yXoFcC/g3cAdgO8AbwdeOuVQkjRvFibwVfU/Z7587FRzSNKiWLglGknS6hh4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1tWHqASRpyZ7v+ezUI7TiHrwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTCxn4JEck+USSS5JcnOSDSQ6cei5JmicLGXhgL+A44BDgIcClwElJdptwJkmaKxumHmAtqurE2a+THAV8nyH4n1x22SZgE8Ae7LleI0rS5BZyDz7J3ZO8I8mZSb4PfIfhZ9l/+XWranNVbayqjbuy+7rPKklTWcg9eOAk4ALgKePna4EtgEs0kjRauMAnuT1wIPCMqvrYuO0BLODPIkm3pkWM4iXARcDRSc4H7gz8OcNevCRptHBr8FV1PfDrwH2BLwOvAZ4PXDXlXJI0bxZxD56qOhm4z7LNt51iFkmaVwu3By9JWh0DL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpjZMPYDUUTb4T2stPnDeqVOPsJB2udPK292Dl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1tebAJ3lfkuN34CySpB1oLvbgk3w8yV9OPYckdTIXgd9Rkuw29QySNC9WFfgkeyY5PskPknwnyfOWXf6fk7w1ySVJrkzykSQ/tew6D0xycpLLk1ya5KNJ9h2XeR4MPCNJjR8HjLc5LMlnk/xw/L6vmI34uOf/V0mOTXIh8Klb+HhIUhur3YM/Fngo8DjgcOD+wGEzlx8PHAr8D+AQ4ArgA0luA5DkYOBjwBnAzwMPBN4FbACeBXwaeAtwp/Hj/CR3Bv4R+Nfx+z0JeDxwzLLZjgQCPAj47eWDJ9mU5NQkp17DVav8cSVp8W24uSskuS1DXJ9YVR8ctx0FfHP88z2BxwAPrqpTxm2/BZwHPAF4I/Bc4LSq2jRz11+d+R5XA1dU1b/PbHs68G3g6VV1PfDVJH8IvD7J86vqivGqZ1fV729t/qraDGwG2Dv71M39vJLUxWr24O8O7Mawlw1AVf0AOH388kDg+mWXXzpeftC46f7AR7dztgOBT49xX/LJcZZ7zGz7/HberyTtFFYT+NyCy2sV19nW/W5tj3t2++VruG9Jam81gT8DuIZh3RyAJHsB9xm/3DLez8/OXL438NPjZQBfAH5xG9/jamCXZdu2AD+bZHbG/zZe98xVzC1JO7WbDfy4HPMm4KVJHjoeHfNmxiBX1b8Bf8ewNv6gJD8N/D/g+8A7xrv5c+D+STYnOTjJTyZ5cpL9x8vPAQ5JckCSO4xRfy2wL/DaJAcmeRTwZ8Bfzqy/S5K2YrVH0TyH4SiYvx0/fxk4Zebyo4DPAX8/ft4TOKKqrgSoqi8CvwTcG/gM8FngNxh+M4DhKJ2rGfbaLwT2r6oLgEcwrN9/keF/Kn8N3OgQTUnSylK18xxYsnf2qUNz+NRjaCeQDTd7gJpW8IHzTp16hIW0y53O+HxVbVy+vdUrWSVJNzDwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU1tmHoAqaO69tqpR1hIj3jkb049woJ60Ypb3YOXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDW18IFPsjFJJTlg6lkkaZ4sfOAlSSsz8JLU1LoFPoPnJjkzyZVJTk9y5MzlB4xLLY9L8uEkVyTZkuShy+7niCRfS/LDJJ8A7rVeP4MkLZL13IN/MfAk4BnAQcAxwOuTPGrZ9V4CvAo4GPgX4G+S3BYgyX7Ae4EPA/cDXg28bB1ml6SFs2E9vkmSvYBnAw+rqk+Mm89OcghD8N8/c/VXVNVJ4+2eB/w2Q8w/CTwNOA94ZlUV8LUk9wL+dBvfexOwCWAP9tyRP5YkzbV1CTzDHvsewAeS1Mz2XYFzll33SzN//tb4+Y7j5wOBz4xxX/LpbX3jqtoMbAbYO/vUtq4rSZ2sV+CXloIezbAHPuuarX1dVZVk9va5VaaTpIbWK/BbgKuAu1TVybfwfh6XJDN78Q+8xdNJUkPrEviquizJscCxGXbJTwFuyxDn68dllNV4HfD7wHFJXgv8NPDUW2NmSVp063kUzfOBFwLPAb7CcCTM44CzV3sHVXUe8FjgCOA04PeAP9zRg0pSB+u1RMO4pPLq8WOly89hhTX2qsqyr9/PjY+6AXj7jplSkvrwlayS1NQO24NPsj/Dk6Bbc9C4xCJJWgc7conmWwwvSNrW5ZKkdbLDAl9V1wJn7Kj7kyTdMq7BS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktTUhqkHkKQl139xy9QjtOIevCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKa2jD1ALe2JJuATQB7sOfE00jS+mm/B19Vm6tqY1Vt3JXdpx5HktZN+8BL0s7KwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekpgy8JDVl4CWpKQMvSU0ZeElqysBLUlMGXpKaMvCS1JSBl6SmDLwkNWXgJakpAy9JTRl4SWrKwEtSUwZekpoy8JLUlIGXpKYMvCQ1ZeAlqSkDL0lNGXhJasrAS1JTBl6SmjLwktSUgZekplJVU8+wbpJcCJw79RxbcQfgoqmHWDA+Zmvj47Y28/y43aWqfmz5xp0q8PMsyalVtXHqORaJj9na+LitzSI+bi7RSFJTBl6SmjLw82Pz1AMsIB+ztfFxW5uFe9xcg5ekptyDl6SmDLwkNWXgJakpAy9JTRl4SWrq/wMqY0v9wmUjsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'father clerk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'you name what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'tomorrow together we go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'today date what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'pink color i like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'apple child eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'this flower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'nine morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'do job what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'school name what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'she say what')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'pain head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "hypothesis = ['I', 'stay', 'in', 'pune']\n",
    "reference = ['I', 'stay', 'in', 'pune']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.635809992474887e-232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = ['father', 'clerk']\n",
    "reference = ['my', 'father', 'is', 'doctor']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  6.968148412761692e-155\n"
     ]
    }
   ],
   "source": [
    "import  nltk.translate.bleu_score as bleu\n",
    "reference_translation=['The cat is on the mat.'.split(),\n",
    "                       'There is a cat on the mat.'.split()\n",
    "                      ]\n",
    "candidate_translation_1='the the the mat on the the.'.split()\n",
    "candidate_translation_2='The cat is on the mat.'.split()\n",
    "print(\"BLEU Score: \",bleu.sentence_bleu(reference_translation, candidate_translation_1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram(unigram, ngram=1):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    -----\n",
    "    counter: dict, containing ngram as key, and count as value\n",
    "    \"\"\"\n",
    "    return Counter(ngrams(unigram, ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram for translation Counter({('the',): 3, ('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('military',): 1, ('always',): 1, ('obeys',): 1, ('commands',): 1, ('of',): 1, ('party.',): 1})\n",
      "Bigram for translation Counter({('It', 'is'): 1, ('is', 'a'): 1, ('a', 'guide'): 1, ('guide', 'to'): 1, ('to', 'action'): 1, ('action', 'which'): 1, ('which', 'ensures'): 1, ('ensures', 'that'): 1, ('that', 'the'): 1, ('the', 'military'): 1, ('military', 'always'): 1, ('always', 'obeys'): 1, ('obeys', 'the'): 1, ('the', 'commands'): 1, ('commands', 'of'): 1, ('of', 'the'): 1, ('the', 'party.'): 1})\n"
     ]
    }
   ],
   "source": [
    "translation = 'It is a guide to action which ensures that the military always obeys the commands of the party.'.split()\n",
    "list_of_references=[\n",
    "    'It is a guide to action that ensures that the military will forever heed Party commands.',\n",
    "    'It is the guiding principle which guarantees the military forces always being under the command of the Party.',\n",
    "    'It is the practical guide for the army always to heed the directions of the party.'\n",
    "\n",
    "]\n",
    "res = count_ngram(translation)\n",
    "print('unigram for translation {}'.format(res))\n",
    "res = count_ngram(translation, 2)\n",
    "print('Bigram for translation {}'.format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('It',): 1,\n",
       "         ('is',): 1,\n",
       "         ('a',): 1,\n",
       "         ('guide',): 1,\n",
       "         ('to',): 1,\n",
       "         ('action',): 1,\n",
       "         ('which',): 1,\n",
       "         ('ensures',): 1,\n",
       "         ('that',): 1,\n",
       "         ('the',): 3,\n",
       "         ('military',): 1,\n",
       "         ('always',): 1,\n",
       "         ('obeys',): 1,\n",
       "         ('commands',): 1,\n",
       "         ('of',): 1,\n",
       "         ('party.',): 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dict()\n",
    "# retrieve translation unigram counts\n",
    "ct_translation_u = count_ngram(translation, ngram=1)\n",
    "ct_translation_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('It', 'is', 'a', 'guide'): 1,\n",
       "         ('is', 'a', 'guide', 'to'): 1,\n",
       "         ('a', 'guide', 'to', 'action'): 1,\n",
       "         ('guide', 'to', 'action', 'which'): 1,\n",
       "         ('to', 'action', 'which', 'ensures'): 1,\n",
       "         ('action', 'which', 'ensures', 'that'): 1,\n",
       "         ('which', 'ensures', 'that', 'the'): 1,\n",
       "         ('ensures', 'that', 'the', 'military'): 1,\n",
       "         ('that', 'the', 'military', 'always'): 1,\n",
       "         ('the', 'military', 'always', 'obeys'): 1,\n",
       "         ('military', 'always', 'obeys', 'the'): 1,\n",
       "         ('always', 'obeys', 'the', 'commands'): 1,\n",
       "         ('obeys', 'the', 'commands', 'of'): 1,\n",
       "         ('the', 'commands', 'of', 'the'): 1,\n",
       "         ('commands', 'of', 'the', 'party.'): 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = dict()\n",
    "# retrieve translation unigram counts\n",
    "ct_translation_b = count_ngram(translation, ngram=4)\n",
    "ct_translation_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_clip_ngram(translation_u, list_of_reference_u, ngram=1):\n",
    "    \"\"\"\n",
    "    Return\n",
    "   clipped counts of the ngram for candidate and reference translation\n",
    "    \n",
    "    \"\"\"\n",
    "    res = dict()\n",
    "    # retrieve hypothesis counts\n",
    "    ct_translation_u = count_ngram(translation_u, ngram=ngram)\n",
    "    \n",
    "    # retrieve translation candidate counts\n",
    "    for reference_u in list_of_reference_u:\n",
    "        ct_reference_u = count_ngram(reference_u, ngram=ngram)\n",
    "        for k in ct_reference_u:\n",
    "            if k in res:\n",
    "                res[k] = max(ct_reference_u[k], res[k])\n",
    "            else:\n",
    "                res[k] = ct_reference_u[k]\n",
    "                \n",
    "\n",
    "    return {\n",
    "        k: min(ct_translation_u.get(k, 0), res.get(k, 0)) \n",
    "        for k in ct_translation_u\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped counts for unigram between candidate and reference {('It',): 1, ('is',): 1, ('a',): 1, ('guide',): 1, ('to',): 1, ('action',): 1, ('which',): 1, ('ensures',): 1, ('that',): 1, ('the',): 3, ('military',): 1, ('always',): 1, ('obeys',): 0, ('commands',): 0, ('of',): 1, ('party.',): 1}\n"
     ]
    }
   ],
   "source": [
    "res = count_clip_ngram(\n",
    "    translation, \n",
    "    list(map(lambda ref: ref.split(), list_of_references))\n",
    ")\n",
    "print(\"Clipped counts for unigram between candidate and reference\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(translation, list_of_references, ngram=1):\n",
    "    \"\"\"\n",
    "    Return\n",
    "    modified precision = clipped counts/ no. of unclipped candidate n-gram\n",
    "    \n",
    "    \"\"\"\n",
    "    ct_clip = count_clip_ngram(translation, list_of_references, ngram)\n",
    "    ct = count_ngram(translation, ngram)\n",
    "    \n",
    "    return sum(ct_clip.values()) / float(max(sum(ct.values()), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modofied precision 0.05555555555555555\n"
     ]
    }
   ],
   "source": [
    "res_mp= modified_precision(translation, list_of_references)\n",
    "print(\"Modofied precision\", res_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_ref_length(translation_u, list_of_reference_u):\n",
    "    \"\"\"\n",
    "    determine the closest reference length from translation length\n",
    "    \"\"\"\n",
    "    len_trans = len(translation_u)\n",
    "    closest_ref_idx = np.argmin([abs(len(x) - len_trans) for x in list_of_reference_u])\n",
    "    return len(list_of_reference_u[closest_ref_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_length = closest_ref_length(translation, list_of_references)\n",
    "ref_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(translation_u, list_of_reference_u):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    c = len(translation_u)\n",
    "    r = closest_ref_length(translation_u, list_of_reference_u)\n",
    "    \n",
    "    if c > r:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(1 - float(r)/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP 0.028565500784550377\n"
     ]
    }
   ],
   "source": [
    "print(\"BP\", brevity_penalty(translation, list_of_references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(translation_u, list_of_reference_u, W=[0.25 for x in range(4)]):\n",
    "    bp = brevity_penalty(translation_u, list_of_reference_u)\n",
    "    modified_precisions = [\n",
    "        modified_precision(translation_u, list_of_reference_u, ngram=ngram)\n",
    "        for ngram, _ in enumerate(W,start=1)\n",
    "    ]\n",
    "    score = np.sum([\n",
    "        wn * np.log(modified_precisions[i]) if modified_precisions[i] != 0 else 0 for i, wn in enumerate(W)\n",
    "    ])\n",
    "    \n",
    "    return bp * np.exp(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU 0.013868315585598298\n"
     ]
    }
   ],
   "source": [
    "print(\"BLEU\", bleu_score(translation, list_of_references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
