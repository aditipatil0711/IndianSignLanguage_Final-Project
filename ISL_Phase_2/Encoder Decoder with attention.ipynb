{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"islcorpus.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Father search what</td>\n",
       "      <td>what is father searching?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>I go theatre</td>\n",
       "      <td>I go to theatre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>you wrong</td>\n",
       "      <td>you are wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>you hungry</td>\n",
       "      <td>are you hungry?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we tomorrow meet</td>\n",
       "      <td>can we meet tomorrow?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Input                     Output\n",
       "84   Father search what  what is father searching?\n",
       "58         I go theatre           I go to theatre.\n",
       "25            you wrong              you are wrong\n",
       "106          you hungry            are you hungry?\n",
       "5      we tomorrow meet      can we meet tomorrow?"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_raw = pd.read_csv(data_path, header = 0)\n",
    "lines_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "#sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "    num_digits= str.maketrans('','', digits)\n",
    "    \n",
    "    sentence= sentence.lower()\n",
    "    sentence= re.sub(\" +\", \" \", sentence)\n",
    "    sentence= re.sub(\"'\", '', sentence)\n",
    "    sentence= sentence.translate(num_digits)\n",
    "    sentence= sentence.strip()\n",
    "    sentence= re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.rstrip().strip()\n",
    "    sentence=  'start_ ' + sentence + ' _end'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_ i see car _end\n",
      "b'start_ i see the car _end'\n"
     ]
    }
   ],
   "source": [
    "isl_sentence = u\"I see car\"\n",
    "eng_sentence = u\"i see the car\"\n",
    "print(preprocess_sentence(isl_sentence))\n",
    "print(preprocess_sentence(eng_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path,encoding = 'UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split(',')]for l in lines[:num_examples]]\n",
    "    print(path)\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islcorpus.csv\n",
      "start_ smoke not _end\n",
      "start_ dont smoke _end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size=125\n",
    "source, target = create_dataset(data_path, sample_size)\n",
    "print(source[-1])\n",
    "print(target[-1])\n",
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
    "source_sentence_tokenizer.fit_on_texts(source)\n",
    "source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n",
    "source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor,padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "target_sentence_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_sentence_tokenizer.fit_on_texts(target)\n",
    "target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n",
    "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding = 'post')\n",
    "print(len(target_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "max_target_length= max(len(t) for t in  target_tensor)\n",
    "print(max_target_length)\n",
    "max_source_length= max(len(t) for t in  source_tensor)\n",
    "print(max_source_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor= train_test_split(source_tensor, target_tensor,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 25 25\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(source_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> start_\n",
      "3 ----> i\n",
      "105 ----> bore\n",
      "2 ----> _end\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> start_\n",
      "4 ----> i\n",
      "8 ----> am\n",
      "123 ----> bore\n",
      "41 ----> doing\n",
      "124 ----> nothing\n",
      "7 ----> .\n",
      "2 ----> _end\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(source_sentence_tokenizer, source_train_tensor[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert( target_sentence_tokenizer, target_train_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(source_train_tensor)\n",
    "BATCH_SIZE = 6\n",
    "steps_per_epoch = len(source_train_tensor)//BATCH_SIZE\n",
    "embedding_dim = 24\n",
    "units = 1024\n",
    "vocab_inp_size = len(source_sentence_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(target_sentence_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([6, 8]), TensorShape([6, 11]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (6, 8, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (6, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (6, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (6, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (6, 184)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 loss 3.0813984870910645\n",
      "Epoch 1 Loss 2.2916\n",
      "Time taken for 1 epoch 100.09047389030457 sec\n",
      "\n",
      "Epoch 2 Batch 0 loss 1.9467170238494873\n",
      "Epoch 2 Loss 2.0363\n",
      "Time taken for 1 epoch 17.845447301864624 sec\n",
      "\n",
      "Epoch 3 Batch 0 loss 1.9316455125808716\n",
      "Epoch 3 Loss 1.9743\n",
      "Time taken for 1 epoch 17.419190645217896 sec\n",
      "\n",
      "Epoch 4 Batch 0 loss 1.5034265518188477\n",
      "Epoch 4 Loss 1.8747\n",
      "Time taken for 1 epoch 18.658706188201904 sec\n",
      "\n",
      "Epoch 5 Batch 0 loss 1.9759126901626587\n",
      "Epoch 5 Loss 1.8201\n",
      "Time taken for 1 epoch 18.30390429496765 sec\n",
      "\n",
      "Epoch 6 Batch 0 loss 1.6149756908416748\n",
      "Epoch 6 Loss 1.7370\n",
      "Time taken for 1 epoch 19.346181631088257 sec\n",
      "\n",
      "Epoch 7 Batch 0 loss 1.8157691955566406\n",
      "Epoch 7 Loss 1.6381\n",
      "Time taken for 1 epoch 17.69764757156372 sec\n",
      "\n",
      "Epoch 8 Batch 0 loss 1.4883036613464355\n",
      "Epoch 8 Loss 1.5667\n",
      "Time taken for 1 epoch 18.505679845809937 sec\n",
      "\n",
      "Epoch 9 Batch 0 loss 1.7359598875045776\n",
      "Epoch 9 Loss 1.4352\n",
      "Time taken for 1 epoch 15.420536518096924 sec\n",
      "\n",
      "Epoch 10 Batch 0 loss 1.0570108890533447\n",
      "Epoch 10 Loss 1.3648\n",
      "Time taken for 1 epoch 17.402713537216187 sec\n",
      "\n",
      "Epoch 11 Batch 0 loss 1.340396523475647\n",
      "Epoch 11 Loss 1.2519\n",
      "Time taken for 1 epoch 14.973370790481567 sec\n",
      "\n",
      "Epoch 12 Batch 0 loss 0.9391101598739624\n",
      "Epoch 12 Loss 1.1558\n",
      "Time taken for 1 epoch 18.194249868392944 sec\n",
      "\n",
      "Epoch 13 Batch 0 loss 1.047275185585022\n",
      "Epoch 13 Loss 1.0544\n",
      "Time taken for 1 epoch 16.403148889541626 sec\n",
      "\n",
      "Epoch 14 Batch 0 loss 0.9076350927352905\n",
      "Epoch 14 Loss 1.0116\n",
      "Time taken for 1 epoch 15.625452756881714 sec\n",
      "\n",
      "Epoch 15 Batch 0 loss 0.6437928080558777\n",
      "Epoch 15 Loss 0.9085\n",
      "Time taken for 1 epoch 15.314433336257935 sec\n",
      "\n",
      "Epoch 16 Batch 0 loss 0.7303904891014099\n",
      "Epoch 16 Loss 0.8840\n",
      "Time taken for 1 epoch 16.562114715576172 sec\n",
      "\n",
      "Epoch 17 Batch 0 loss 0.9000167846679688\n",
      "Epoch 17 Loss 0.8519\n",
      "Time taken for 1 epoch 18.27088713645935 sec\n",
      "\n",
      "Epoch 18 Batch 0 loss 0.5604190826416016\n",
      "Epoch 18 Loss 0.7793\n",
      "Time taken for 1 epoch 18.56226134300232 sec\n",
      "\n",
      "Epoch 19 Batch 0 loss 0.481232225894928\n",
      "Epoch 19 Loss 0.7212\n",
      "Time taken for 1 epoch 18.516172885894775 sec\n",
      "\n",
      "Epoch 20 Batch 0 loss 0.66434645652771\n",
      "Epoch 20 Loss 0.6759\n",
      "Time taken for 1 epoch 16.8868145942688 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} loss {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
    "   \n",
    "      \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_target_length, max_source_length))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    #print(sentence)\n",
    "    #print(source_sentence_tokenizer.word_index)\n",
    "\n",
    "    inputs = [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_source_length,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
    "\n",
    "    for t in range(max_target_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '_end':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "  \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1903502e190>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: start_ child angry why _end\n",
      "Predicted translation: good night _end \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-71f02795d875>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "<ipython-input-57-71f02795d875>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAH/CAYAAAASb3qiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb1klEQVR4nO3deZSlB1nn8d+TBULQqCgga6JohLgAQ9hUQAElynEZcBkHRNyiCEcFFQRkc2BEBkZFhwM5uKCMiuICiIIwGEAHZVEgiMIESABzFCIoSxayPPPHe5tUyu50dbrS97nVn885OVV131u3njpv+t5vvdut7g4AADMcs+4BAAC4ijgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDjbcFV1TlXdat1zwF5RVW+tqkdU1eesexbg6CTONt8pSY5f9xCwh7w8yaOTXFBVv1NV91n3QMDRRZwBbNHdj09ycpIHJDk2ycur6ryqemJV3Xq90wFHA3EGsE0v/qy7vyPJzZM8L8njkry3ql5ZVWesd0JgLxNnAAdQVXdL8vQkP53kgiRPSfKeJC+uql9c42jAHnbcugcAmKSqbpLkIUm+N8ltkrw0ybd196u23OfFq9t/fB0zAnubOAO4ug8mOTfJryZ5QXdfuJ/7vDnJm47oVLDHHMoxnN39/utylmmqu9c9A/tRVQ9J8qLuvnTb7ddL8l+6+zdXX//XJC/p7k+uYUxWPMnsDVV1TJKvSvJ33f2Jdc8De1lVXZlkRxHS3cdex+OMIs6Gqqorktysuz+07fbPTfKho+1/1Ok8yewNVVVJLk1yWnefu+55YC+rqjtt+fLUJM9I8twkb1jddvckP5TkMd39O0d4vLWyW3Ouyv5f7G+d5N+P8Cwc3J23fH6NTzJHeC4OQXd3Vb0ryY2z7NoEriPd/ZZ9n1fV/0zyyO5+8Za7vGb17/HHkhxVcWbL2TBVdU6WKPvSJO9KcvmWxcdmuf7Sn65O8Wegqnptkl/e9iSTqvq2JD/W3fdYz2TsRFV9Q5LHJ3lEkre1J0m4zlXVxUlu393v3nb7qUne2t0nrmey9bDlbJ59L+hfluVK5VuPe/lUkvOS/MERnolDc5ckb9/P7W9Pcqf93M4sv5fkhCRvSXJ5VV3tuM/uPmktU7EjVXXj7v7wuufgkJ2X5EfyH8+A/pEk5x/pYdZNnA3T3U+pquOSXJjkj7v7n9Y9E4fsvHiS2WSPWPcAHJZ/qqqXZjnb9hW2fG6MRyb5o9UFnv96ddtds7xF4QPWNdS62K05VFVdkuS23X3eumfh0KyeXP4oS4j9hyeZ7v6zNY0Ge15VfV2Wa9R9a5KPJPn1JL/R3e9Z51wcXFXdMssfsbfNctz1O5M8t7s/sNbB1kCcDVVVf5Pk8d396nXPwqGrqlsleVg8yWyca7gsSie5xC6zzVBVn53kQVlC7Y5JXptla9ofdPclaxwNDkqcDbU6KPnpSZ6U5diXq13HrLs/so65YK/bwWVRPpZla8yju/vya7gfQ1TVw5M8K8n1kvxbkrOSPNW17GapqhOT3CHJTbLt7SW7+w/XMdO6iLOhVi8Q+2xdSZXljH/Xyhqkqv7TTu/b3X97Xc7C4amq78xVl0L5m9XNd01yZpInJ/nsJD+T5Dnd/aQ1jMgOVNXNknxPli1nt8hystWvZnkj+8cmubC777u+Cdmqqu6b5XIZn7ufxUfda544G6qq7nVNy7v7tUdqFg5uy9aWOshdj7onmU1TVWcnefb2v9Sr6gFZLoVyr6r6riRP6e5T1zEjB7ZaT9+X5OuTvCPJ85O8sLs/tuU+p2W5PMP11jMl21XV32d5S7THdfcF655n3ZytOZT42jhfsO4B2DV3TXLOfm5/R6662PAbktzyiE3Eofj1LFtg7r71IqfbvC/J047cSOzAKUm+WZgtxNlwVXXzLO8KcLW/8Lr7deuZiP3pbpfI2DvOz7IL86e23f6DSfa9L+qNs5wJyDw36+6LrukO3X1xkqccoXnYmb9K8iVJnFUbcTbWKsp+O8k9c9Xusq37oO0aG2R1zNlbu/vKgx1/5piz8X4iyR9U1Tdm2c3SWbaY3SbJA1f3uXOWi9UyzL4wWz2H7u/Acv/+Znpukmeu1ts5SS7buvBoW2+OORuqqn4vy4GRD8/yAnFGkpsm+dks7z/2qjWOxzarY84+v7s/dJDjzxxztgFWl0L5kSx/yVeSf8hyKZT3X+M3snZVdcckL8xVl7HZyr+/obadBLfdUbfexNlQVfUvSe7f3W+uqo8lOb27311V90/yhO6+25pHZIuqOjnJ+1dvnH3yNd3XLlC47lTVm5L8a5Y/ZC/Itsui+Pc3k+fNq7Nbc64bZHkLp2Q5tuUmSd6d5WKmX7Guodi/rU8cR9uTyF7keksb7bQkd9z+BtrM5nnz6sTZXP+YZbP8eUnemuSHq+oDWXZzer/N4by4b66DXW8pjvec7pwkn5/lj1k2yOri6w9P8oVJ7tfdH6iqH0jyvu7+P+ud7sgSZ3P9UpYnmGTZPP+KJN+V5NIsF1ZkKC/uG++Xkrw8rre0MarqRlu+fFySZ1TVz2T/B5Y7y3agqnpQlpMCnp/kPkmOXy06NsmjkxxVceaYsw2x2hJz2yzHNV14sPuzPi6muNmq6pNJvsIbZW+O/bzl1r4TAby7yoaoqrcl+bnu/t2q+niS23f3e6vq9kn+vLtvuuYRjyhbzoaqqicmeea+08JXH/+2qm5QVU/s7p9d74Rcg1PiYoqbzPWWNs/XrnsADtsXZ7m483afSHLSEZ5l7cTZXE/Ksol3+8UUT1wtE2dzeXHfbK63tGG2vqNKVb0yydmr/97Y3VesaSwOzQVJTs1yEeit7pmj8LlUnM21/aKz+9wxrkw+zrYLz3px32wvXn08az/LHDM435uS3D/Lm9R/qqr+b8TaJjgrybNXJwAkya2q6h5JnpFlXR5VHHM2zGpfeye5YZatZtvfFeCELBfDfPgaxuMAvPH53uF6S3tDVd0gyVcl+ZrVf3dJckl3H3W7yDZFVT0tySOzvM4lywlwz+zuJ2y5zy2TXNDd13TR2o0nzoapqu/J8gL/a0l+PMm/b1n8qSTndff+9suzRgd7Qd/Ki/t8VXVclhfz7e9r2939W+uZikNRVTfNEmX3znJM2q2S/HV3Oz5tsNXJb6dluQTRO7v7E9uWfyzJHbr7veuY70gRZ0NV1cOTvK67z1l9/XVZLqHx90meYdP8XKu//j7Q3c/ddvsPJ7nF1r8CmaeqbpvkZUm+IMsfSldkOQTksiSX2vIyW1X9rywxdnKSNyZ5bZZdmm/o7kvXOBq7YOuZnOue5bp0zMHvwpo8OMmXJp/ejPvHSW6U5QJ9T13fWOzAdyf5u/3c/pYkDznCs3DofjHLuvqsLIcW3C7J6VkuBv3AA34XUzwsyzUGn57kp5I8pbvPFmZsEnE21+2S7Dtw/NuzHMj6jVle+L9rbVOxEzdJ8uH93P6vWd68ntnunOSp3f3JJFcmOW51EsejkzxrrZOxE6cmeXyWM6b/KMlHquplVfWobSfuwFjibK5jsxxjlixXS/7T1efviRf46d6f5B77uf2eST54hGfh0FWuuoTNh5PcYvX5B5N80VomYse6+9zufn53P7i7b5XkK7O8T/HPZzmTE8ZzKY253pHkYVX1J1ni7LGr22+Rq94QnZmel+QXqup6SV6zuu0+SX4uywsEs70jye2TvDfLMUuPqaorkvxgknPXORgHV1XHZNkN/bVZTgj4qixn/70lyV+sbzJ2yVFxoLw4m+sxWY4z+8kkL9h3YkCSb87ygsFQ3f2sqvq8JM/OVWf6fSrJL3X3M9Y3GTv0tCyXskmSn0nyJ1le1C9M8h3rGood+7ck189y3OfZWd4r9fWr3dRsvoNdrmhPcLbmYFV1bJKTuvujW247JclF3f2htQ3GjlTVDbOcEl7ZzynhbI7VG2t/tD1hjldVZ0SM7VlVdass1znb01csEGcAAIM4IQAAYBBxtiGq6sx1z8C1Z/1tLutus1l/m+1oXX/ibHMclf+D7iHW3+ay7jab9bfZjsr1J84AAAbZMycEXK+u3yd8+uz3veeyXJrjc/11j8G1ZP1tLutus1l/m20vr7+P56MXdveN97dsz1zn7ITcMHc95r7rHgNgs+yRP9CPSsccu+4JOAyvvuJF5x9omd2aAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDjI+zqnpHVT153XMAABwJ4+MMAOBoIs4AAAbZcZxV1Q2r6jer6hNV9S9V9diq+pOq+o3V8s+pqhdU1Uer6uKqenVVfem2x3hAVZ1TVZdW1Qeq6vFVVVuW36SqXrL6/vOr6vt27TcFANgAh7Ll7FlJ7pXkPye5d5LbJ7nHluW/keSuSb4lyV2SXJTkFVV1gySpqjsl+f0kf5jky5P8dJLHJnnEtsf4oiT3TfKtSR6S5JRD+YUAADbZcTu5U1V9RpLvS/KQ7n7V6rbvT/LB1edfnOSbk9yru1+3uu27k7w/yYOSPD/Jo5K8truftHrYd6++7zFJfrmqTk3yDUm+urv/avUY35Pkvdcw15lJzkySE3LiIfzaAAAz7XTL2W2SHJ/kjftu6O5PJnnH6svbJbkyyRu2LP/3JOckOW3Lff5q2+P+ZZJbVNVJWx5j6884P8kFBxqqu8/q7tO7+/Tjc/0d/ioAAHPtNM72HRfWB1m+P73lPgf6/j7IYwAAHBV2GmfnJrksy7FkSZKqOjHJl62+fOfqse6+ZflJWY4te+eW+3z1tsf96iQf7O6PJ/mH1WPcectj3DrJzXc4IwDAxttRnHX3J5L8WpKfr6r7VNVpWY4jO2ZZ3P8vyUuSPK+q7lFVX57khUk+luS3Vw/zrCT3qqonV9WpVfWgJD+R5Bmrn/GuJK9YPcbdq+oOWU4QuHh3flUAgPkO5WzNn0zy+iQvTfIXSd6e5M1JLlkt/94sx4u9dPXxxCRndPfFSdLdf5vk25M8MMuxak9f/fcrW37GQ5O8L8lrkrwsS9idd8i/FQDAhqruAx0GdpBvrLp+kvOT/I/uftauTnUtnFQ36rsec991jwGwWa7lawADHHPsuifgMLz6ihe9pbtP39+yHV1KI0mq6o5Zzqh8Y5LPzHIJjM9M8qLdGBIAgEOIs5VHJfmSJJcneWuSe3b3B3d7KACAo9WO46y7/y7Jfje/AQCwO7zxOQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAxy3LoH2C2X3eaE/NMzT1v3GHDUOfaYK9c9Aofj7M9Z9wRcS2979HPWPQKH4dibHXiZLWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQQ47zqrq7Kr6lUP8nq6qbzvcnw0AsNcctwuP8YAkl+3C43xaVZ2S5H1J7tzdb97NxwYAmOyw46y7P7IbgwAAsIPdmqvdls+pqv9eVRdW1Yeq6plVdcyW5b+y5f43raqXVtXFVXV+VX1vVb2jqp687aFvVFW/X1WfrKr3VtWDtyx73+rjm1a7QM8+zN8TAGAj7PSYswcluTzJVyZ5RJIfT/KdB7jvC5KcnOTeSb4lyYNXX2/3xCQvSXL7JC9K8mtVte9+d1l9PCPJzbLsOgUA2PN2Gmfv7O4ndve7u/v3kvxFkvtsv1NVfUmS+yX5oe5+Q3e/NclDk5y4n8f8re5+YXefm+QJWeLvHqtlH159/Nfu/ucD7TqtqjOr6s1V9ebLP3bRDn8VAIC5dhpnb9/29QVJbrKf+902yZVJPn0Qf3d/YHX/Az5md1+eJcj295gH1N1ndffp3X36cSftr/8AADbLTuNs+9mYfYDvrUP42Tt9TACAo8Zux9A/rB7zTvtuqKpbJrn5IT7Op1Yfj92luQAANsKuxll3vyvJK5M8t6ruVlV3SPLrSS7KsmVspz6U5OIk91ud/flZuzknAMBU18VuxIcm+WCSs5O8NMn/zhJbl+z0AVbHoP1okh/IcrzaS3Z7SACAiQ56Edru/pr93PbQAy3v7n9O8k37vq6qz0tyVpJzt9znPxyb1t2nbPv6+Umef7D5AAD2kt14+6arqap7J/nMJOdkOfvyaUkuTPKK3f5ZAAB7za7HWZLjkzw1yRdmOdbsb5Lcs7s/eR38LACAPWXX46y7X5nlpAAAAA6R64oBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIOIMAGAQcQYAMIg4AwAYRJwBAAwizgAABhFnAACDiDMAgEHEGQDAIMete4Ddcvx7L8ktv+Nd6x4DYKP0FVesewSupW846yvXPQKH5dwDLrHlDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGEScAQAMIs4AAAYRZwAAg4gzAIBBxBkAwCDiDABgEHEGADCIOAMAGGR8nFXV6VXVVXXKumcBALiujY8zAICjiTgDABhk1+KsFo+uqvdU1cVVdU5VPXjL8lNWuycfWFWvqqqLquqdVfV12x7njKr6x6q6pKpen+TU3ZoRAGC63dxy9tQk35/k4UlOS/JzSZ5XVfffdr+nJXl2ktsneVOS362qz0iSqrpVkj9O8qokd0jyy0mesYszAgCMdtxuPEhV3TDJo5J8fXe/fnXz+6rqLlli7eVb7v4L3f2y1fc9LslDsoTYXyZ5WJL3J/nR7u4k/1hVpyb5bwf4uWcmOTNJTsiJu/GrAACs1a7EWZYtZSckeUVV9Zbbj09y3rb7vn3L5xesPt5k9fF2Sf56FWb7vOFAP7S7z0pyVpKcdMyN+kD3AwDYFLsVZ/t2j35Tli1fW112oK+7u6tq6/fXLs0DALCRdivO3pnk0iQnd/drDvNxHlhVtWXr2d0OezoAgA2xK3HW3R+vqmcmeWYtm8Jel+QzsoTVlavdjzvx3CQ/keQXq+o5Sb48yQ/vxowAAJtgN8/WfEKSJyf5ySR/n+WMywcmed9OH6C735/kAUnOSPK2JI9M8tO7OCMAwGh19WPvN9dJx9yo73bc/dY9BsBG6SuuWPcIXEvH3OAG6x6Bw/Dnn/zNt3T36ftb5h0CAAAG2fExZ1V16ywH7B/IaavdkgAAXEuHckLABVkuFntNywEAOAw7jrPuvjzJudfhLAAARz3HnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQY5b9wC7ppO+/PJ1TwEAR8SVF1207hG4jthyBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYJDj1j3A4aiqM5OcmSQn5MQ1TwMAcPg2estZd5/V3ad39+nH5/rrHgcA4LBtdJwBAOw14gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADCLOAAAGEWcAAIOIMwCAQcQZAMAg4gwAYBBxBgAwiDgDABhEnAEADFLdve4ZdkVVfTjJ+eue4zr0eUkuXPcQXGvW3+ay7jab9bfZ9vL6O7m7b7y/BXsmzva6qnpzd5++7jm4dqy/zWXdbTbrb7MdrevPbk0AgEHEGQDAIOJsc5y17gE4LNbf5rLuNpv1t9mOyvXnmDMAgEFsOQMAGEScAQAMIs4AAAYRZwAAg4gzAIBB/j+/pguC4uaSXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'child angry why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
