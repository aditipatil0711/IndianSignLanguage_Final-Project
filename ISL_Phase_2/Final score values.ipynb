{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Good', 'morning.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']], [['Wait,', 'I', 'am', 'thinking.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']], [['Wait,', 'I', 'am', 'thinking.']], [['Please', 'call', 'ambulance.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']], [['Wait,', 'I', 'am', 'thinking.']], [['Please', 'call', 'ambulance.']], [['No', 'smoking', 'please.']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']], [['Wait,', 'I', 'am', 'thinking.']], [['Please', 'call', 'ambulance.']], [['No', 'smoking', 'please.']], [['Will', 'he', 'go', 'to', 'school?']]]\n",
      "[[['Good', 'morning.']], [['Where', 'is', 'the', 'bank?']], [['Look', 'at', 'me', 'singing!']], [['It', 'is', '9', 'in', 'the', 'morning.']], [['I', 'want', 'food.'], ['I', 'am', 'hungry.']], [['I', 'am', 'understanding.']], [['I', 'understood.']], [['I', 'am', 'not', 'understanding.']], [['Where', 'are', 'you', 'going', 'today?']], [['This', 'iphone', 'costs', 'seventy', 'thousand.']], [['I', 'am', 'really', 'woried', 'because', 'of', 'rain.']], [['Family', 'does', 'not', 'help', 'me.']], [['I', 'am', 'sorry.']], [['I', 'am', 'engineer.']], [['Wait,', 'I', 'am', 'thinking.']], [['Please', 'call', 'ambulance.']], [['No', 'smoking', 'please.']], [['Will', 'he', 'go', 'to', 'school?']], [['I', \"haven't\", 'had', 'tea', 'yet.']]]\n"
     ]
    }
   ],
   "source": [
    "a_file = open(\"refe.txt\", \"r\")\n",
    "def cloning(li):\n",
    "    new_li=[]\n",
    "    for item in li:\n",
    "        new_li.append(item)\n",
    "    return new_li    \n",
    "\n",
    "layer1 = []\n",
    "layer2 = []\n",
    "\n",
    "\n",
    "for line in a_file:\n",
    "    stripped_line = line.strip()\n",
    "    #print(stripped_line)\n",
    "    line_list = stripped_line.split()\n",
    "    if line_list[-1]!='<eos>':\n",
    "        layer2.append(line_list)\n",
    "    else:\n",
    "        layer2.append(line_list[:len(line_list)-1])\n",
    "        \n",
    "    #print(layer2)\n",
    "    if line_list[-1]=='<eos>':\n",
    "       \n",
    "        layer1.append(cloning(layer2))\n",
    "        print(layer1)\n",
    "        layer2.clear()\n",
    "        #print(layer1)\n",
    "        \n",
    "#print(layer1)       \n",
    "\n",
    "#layer1 ->[[['I', 'want', 'food'], ['I', 'want', 'the', 'food', '<eos>']]]\n",
    "#layer1 ->[[]] ->[layer2]\n",
    "#layer2 ->[['I', 'came', '<eos>']]\n",
    "#layer1 ->[[['I', 'came', '<eos>']]]\n",
    "#layer1 ->[[],[]]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'is', 'in', 'the', 'afternoon'], ['where', 'is', 'the', 'bank'], ['look', 'at', 'me', 'singing', '!'], ['i', 'am', 'hurt'], ['i', 'want', 'food'], ['i', 'understand'], ['i', 'do', 'not', 'understand'], ['i', 'was', 'painter'], ['where', 'are', 'you', 'going', 'today?'], ['i', 'like', 'blue'], ['i', 'am', 'really', 'woried', 'because', 'of', 'rain'], ['i', 'have', 'to', 'me'], ['i', 'am', 'sorry'], ['i', 'am', 'engineer'], ['wait', 'i', 'am', 'thinking'], ['please', 'call', 'an', 'ambulance'], ['no', 'smoking', 'please'], ['will', 'he', 'go', 'to', 'school'], ['i', 'havenâ€˜t', 'had', 'tea', 'yet']]\n"
     ]
    }
   ],
   "source": [
    "b_file = open(\"predict.txt\", \"r\")\n",
    "\n",
    "\n",
    "list_of_lists_ = []\n",
    "\n",
    "for line in b_file:\n",
    "    stripped_line = line.strip()\n",
    "    line_list = stripped_line.split()\n",
    "    list_of_lists_.append(line_list)\n",
    "\n",
    "\n",
    "b_file.close()\n",
    "\n",
    "\n",
    "print(list_of_lists_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=1,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32459051375236697,\n",
       " [0.32894736842105265],\n",
       " 0.9867551618071956,\n",
       " 0.9868421052631579,\n",
       " 75,\n",
       " 76)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_bleu(layer1,list_of_lists_,1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18377079424627574\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "score = corpus_bleu(layer1,list_of_lists_)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "18\n",
      "0.18377079424627574\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "print(len(layer1))\n",
    "print(len(list_of_lists_)-1)\n",
    "score = corpus_bleu(layer1,list_of_lists_)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.20642201834862386\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.gleu_score as gleu\n",
    "sc=gleu.corpus_gleu(layer1,list_of_lists_)\n",
    "print(\"score: {}\".format(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def corpus_meteor(expected, predicted):\n",
    "    meteor_score_sentences_list = list()\n",
    "    [meteor_score_sentences_list.append(meteor_score(expect, predict)) for expect, predict in zip(expected, predicted)]\n",
    "    meteor_score_res = np.mean(meteor_score_sentences_list)\n",
    "    return meteor_score_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_meteor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good morning.', 'Where is the bank?', 'Look at me singing!', 'It is 9 in the morning.', 'I want food.', 'I am understanding.', 'I understood.', 'I am not understanding.', 'Where are you going today?', 'This iphone costs seventy thousand.', 'I am really woried because of rain.', 'Family does not help me.', 'I am sorry.', 'I am engineer.', 'Wait, I am thinking.', 'Please call ambulance.', 'No smoking please.', 'Will he go to school?', \"I haven't had tea yet.\"]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "b_file = open(\"reference.txt\", \"r\")\n",
    "\n",
    "\n",
    "reference= []\n",
    "\n",
    "for line in b_file:\n",
    "    stripped_line = line.strip()\n",
    "    reference.append(stripped_line)\n",
    "\n",
    "\n",
    "b_file.close()\n",
    "\n",
    "\n",
    "print(reference)\n",
    "print(len(reference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it is in the afternoon', 'where is the bank', 'look at me singing !', 'i am hurt', 'i want food', 'i understand', 'i do not understand', 'i was painter', 'where are you going today?', 'i like blue', 'i am really woried because of rain', 'i have to me', 'i am sorry', 'i am engineer', 'wait i am thinking', 'please call an ambulance', 'no smoking please', 'will he go to school', \"i haven't had tea yet\"]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "b_file = open(\"predict.txt\", \"r\")\n",
    "\n",
    "\n",
    "model_out = []\n",
    "\n",
    "for line in b_file:\n",
    "    stripped_line = line.strip()\n",
    "    model_out.append(stripped_line)\n",
    "\n",
    "\n",
    "b_file.close()\n",
    "\n",
    "\n",
    "print(model_out)\n",
    "print(len(model_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.0.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\lenovo\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rouge) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.8333333286904762,\n",
       "  'p': 0.8333333333333333,\n",
       "  'r': 0.8333333333333333},\n",
       " 'rouge-2': {'f': 0.7976190429761905,\n",
       "  'p': 0.7976190476190476,\n",
       "  'r': 0.7976190476190476},\n",
       " 'rouge-l': {'f': 0.8333333286904762,\n",
       "  'p': 0.8333333333333333,\n",
       "  'r': 0.8333333333333333}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "rouge.get_scores(model_out[:len(model_out)-3], reference[:len(reference)-1], avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.41904761563843484,\n",
       "  'p': 0.41441102756892234,\n",
       "  'r': 0.4258145363408522},\n",
       " 'rouge-2': {'f': 0.30451127530612254,\n",
       "  'p': 0.3026315789473684,\n",
       "  'r': 0.30701754385964913},\n",
       " 'rouge-l': {'f': 0.41904761563843484,\n",
       "  'p': 0.41441102756892234,\n",
       "  'r': 0.4258145363408522}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
